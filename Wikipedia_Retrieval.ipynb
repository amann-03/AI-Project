{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsj69ZbjGU7s",
        "outputId": "067a75ab-49b5-4796-8aa4-91035587a502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=e220f36a4f1005950a28acc60a96054e82f424acb01e8c1b0b7264289daa0037\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from urllib.parse import quote"
      ],
      "metadata": {
        "id": "7HfTYaC-GjJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from urllib.parse import quote\n",
        "\n",
        "def search_wiki_pages(keywords):\n",
        "    relevant_pages = []\n",
        "    for keyword in keywords:\n",
        "        try:\n",
        "            # Search for pages related to the keyword\n",
        "            page = wikipedia.page(keyword)\n",
        "            relevant_pages.append((keyword, page.url))\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            # If the keyword doesn't match any page, skip it\n",
        "            print(f\"No Wikipedia page found for '{keyword}'\")\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            # If multiple pages match the keyword, choose the first one\n",
        "            possible_pages = e.options\n",
        "            if possible_pages:\n",
        "                first_page = wikipedia.page(possible_pages[0])\n",
        "                relevant_pages.append((keyword, first_page.url))\n",
        "            else:\n",
        "                print(f\"No relevant Wikipedia page found for '{keyword}'\")\n",
        "    return relevant_pages"
      ],
      "metadata": {
        "id": "TFk7nnNVq0Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo KeyWords"
      ],
      "metadata": {
        "id": "78m4GaGaFj1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [\"Python Programming Language\", \"MachineLearning\", \"Artificial Intelligence\",\"Cloud Security\",\" BFS Algorithm\"]\n",
        "relevant_pages = search_wiki_pages(keywords)\n",
        "for keyword, url in relevant_pages:\n",
        "    print(f\"Keyword: {keyword}\")\n",
        "    print(f\"URL: {url}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZEu6ImlrLHu",
        "outputId": "11c775fd-a843-4d8a-d4be-5fed83870245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyword: Python Programming Language\n",
            "URL: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
            "\n",
            "Keyword: MachineLearning\n",
            "URL: https://en.wikipedia.org/wiki/Machine_learning\n",
            "\n",
            "Keyword: Artificial Intelligence\n",
            "URL: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "\n",
            "Keyword: Cloud Security\n",
            "URL: https://en.wikipedia.org/wiki/Cloud_computing_security\n",
            "\n",
            "Keyword:  BFS Algorithm\n",
            "URL: https://en.wikipedia.org/wiki/Breadth-first_search\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PedzpwYCsb6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}